{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baa85158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec7e6279",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('train_image.csv', header = None)\n",
    "training_label = pd.read_csv('train_label.csv', header = None)\n",
    "test_data = pd.read_csv('test_image.csv', header = None)\n",
    "test_label = pd.read_csv('test_label.csv', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a57cb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(training_data)\n",
    "y = np.array(training_label)\n",
    "X_test = np.array(test_data)\n",
    "y_test = np.array(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79079fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 1)\n"
     ]
    }
   ],
   "source": [
    "# plt.imshow(X[0].reshape((28,28)), cmap='gray')\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2c5230d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = (X/255).astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "618ae695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotEncode(y):\n",
    "    ans = []\n",
    "    for i in range(len(y)):\n",
    "        currentLabel = np.zeros(10)\n",
    "        currentLabel[y[i]] = 1\n",
    "        ans.append(currentLabel)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73029145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "y_train_encoded = oneHotEncode(y)\n",
    "y_test_encoded = oneHotEncode(y_test)\n",
    "print(np.array(y_train_encoded).shape, np.array(y_test_encoded).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "996727ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_encoded = oneHotEncode(y)\n",
    "# print(np.array(y_encoded).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02d65172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1. / (1. + np.exp(-x))\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x - x.max()) / np.sum(np.exp(x - x.max()), axis = 0, keepdims = True)\n",
    "\n",
    "def CalculateLoss(yHat, y):\n",
    "#     yHat = yHat*y\n",
    "#     L_sum = np.sum(-1 * np.multiply(y, np.log(yHat + 1e-8)))\n",
    "    L_sum = np.sum(np.multiply(y, np.log(yHat)) + np.multiply((1 - y), np.log(1 - yHat)))\n",
    "    m = y.shape[1]\n",
    "    L = -(1./m) * L_sum\n",
    "\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52184f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(X, y, trainSize = 0.85):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    lenTrain = len(X) * trainSize\n",
    "    X_copy = list(X)\n",
    "    y_copy = list(y)\n",
    "    while len(X_train) < lenTrain:\n",
    "        ind = randrange(len(X_copy))\n",
    "        X_train.append(X_copy.pop(ind))\n",
    "        y_train.append(y_copy.pop(ind))\n",
    "    return X_train, X_copy, y_train, y_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e46dde30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = splitData(X, y_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "753062fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "#     Run this cell only once.\n",
    "#     Else, it will transpose X multiple times and we won't get the desired results.\n",
    "# \"\"\"\n",
    "# X_train = (np.array(X_train)).T\n",
    "# y_train = (np.array(y_train)).T\n",
    "# X_test = (np.array(X_test)).T\n",
    "# y_test = (np.array(y_test)).T\n",
    "# print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "# print(np.random.permutation(X_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c588a9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 60000) (10, 60000) (784, 10000) (10, 10000)\n",
      "[31076 33930 20180 ... 49761  2643  9346]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Run this cell only once.\n",
    "    Else, it will transpose X multiple times and we won't get the desired results.\n",
    "\"\"\"\n",
    "X_train = (np.array(X)).T\n",
    "y_train = (np.array(y_train_encoded)).T\n",
    "X_test = (np.array(X_test)).T\n",
    "y_test = (np.array(y_test_encoded)).T\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "print(np.random.permutation(X_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88eb77ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "nnL1 = 256\n",
    "nnL2 = 128\n",
    "numFeat = 784\n",
    "classes = 10\n",
    "weightsAndBiases = {\n",
    "    'W1' : np.random.randn(nnL1, numFeat) * 0.01,\n",
    "    'W2' : np.random.randn(nnL2, nnL1) * 0.01,\n",
    "    'W3' : np.random.randn(classes, nnL2) * 0.01,\n",
    "    'b1' : np.zeros(nnL1).reshape(nnL1,1),\n",
    "    'b2' : np.zeros(nnL2).reshape(nnL2,1),\n",
    "    'b3' : np.zeros(classes).reshape(classes,1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "949fdfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardProp(X):\n",
    "#     print(weightsAndBiases)\n",
    "    W1 = weightsAndBiases['W1']\n",
    "    W2 = weightsAndBiases['W2']\n",
    "    W3 = weightsAndBiases['W3']\n",
    "    b1 = weightsAndBiases['b1']\n",
    "    b2 = weightsAndBiases['b2']\n",
    "    b3 = weightsAndBiases['b3']\n",
    "#     print(W1.shape, X.shape, b1.shape)\n",
    "    z1 = np.dot(W1, X) + b1\n",
    "    o1 = sigmoid(z1)\n",
    "#     print(outputLayer1)\n",
    "\n",
    "#     print(z1.shape)\n",
    "    z2 = np.dot(W2, o1) + b2\n",
    "    o2 = sigmoid(z2)\n",
    "    z3 = np.dot(W3, o2) + b3\n",
    "    o3 = softmax(z3)\n",
    "    importantValuesForBackPropagation = []\n",
    "    importantValuesForBackPropagation.append((z1, o1))\n",
    "    importantValuesForBackPropagation.append((z2, o2))\n",
    "    importantValuesForBackPropagation.append((z3, o3))\n",
    "    \n",
    "    return importantValuesForBackPropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6446024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backProp(X, Y, z_a_values, batchSize):\n",
    "    yHat = z_a_values[2][1]\n",
    "    O2 = z_a_values[1][1]\n",
    "    z2 = z_a_values[1][0]\n",
    "    z1 = z_a_values[0][0]\n",
    "    O1 = z_a_values[0][1]\n",
    "#     loss = CrossEntropy(yHat, Y)\n",
    "    \"\"\"\n",
    "    https://towardsdatascience.com/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1\n",
    "    dLossByZ3 = (dCost/dOut3) * (dOut3/dZ3)\n",
    "    dLossBydW3 = dLossBydOutput * (dZ3/dW3)\n",
    "    \n",
    "    dLossBydOutput2 = dLossBydOutput * (dZ3/dOut2)\n",
    "    dLossBydW2 = dLossBydOutput2 * (dOut2/dZ2) * (dZ2/dW2)  [dOut2/dZ2 = sigmoid(z2) * (1-sigmoid(z2))]\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    dZ3 = yHat - Y #+ (1-Y)/(1-yHat)\n",
    "    \n",
    "#     print(dLossBydOutput.shape)\n",
    "#     print(outputLayer2.T.shape)\n",
    "    \n",
    "    dW3 = (1/batchSize) * np.dot(dZ3, O2.T)\n",
    "    db3 = (1/batchSize) * np.sum(dZ3, axis=1, keepdims=True)\n",
    "    \n",
    "    \n",
    "    dO2 = np.dot(weightsAndBiases['W3'].T, dZ3)\n",
    "    \n",
    "    \n",
    "#     print(dLossBydOutput2.shape, sigmoid(z2).shape)\n",
    "\n",
    "    dZ2 = dO2 * sigmoid(z2) * (1-sigmoid(z2))\n",
    "    \n",
    "    dW2 = (1/batchSize) * np.dot(dZ2, O1.T)\n",
    "    db2 = (1/batchSize) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "        \n",
    "#     print(dLossBydOutput2.shape, dO2BydZ2.shape)\n",
    "#     dLossBydOutput1 = (dLossBydOutput2 * dO2BydZ2)\n",
    "\n",
    "    dO1 = np.dot(weightsAndBiases['W2'].T, dZ2) \n",
    "    dZ1 = dO1 * sigmoid(z1) * (1-sigmoid(z1))\n",
    "\n",
    "    \n",
    "    dW1 = (1/batchSize) * np.dot(dZ1, X.T)\n",
    "    db1 = (1/batchSize) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    weightDerivatives = [dW1, dW2, dW3]\n",
    "    biasDerivatives = [db1, db2, db3]\n",
    "    \n",
    "    return weightDerivatives, biasDerivatives\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f2758ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracyScore(x,y):\n",
    "    yHat = forwardProp(x)\n",
    "    yHat = yHat[2][1]\n",
    "    yHat = yHat.argmax(axis=0)\n",
    "    y = y.argmax(axis=0)\n",
    "    c1 = (yHat==y)*1\n",
    "    return np.mean(c1) * 100\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a984028a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, y_train, X_test, y_test, epochs, batchSize, learningRate):\n",
    "    numImg = X_train.shape[1]\n",
    "    print('num: ',numImg)\n",
    "    for epoch in range(epochs):\n",
    "#         permutation = np.random.permutation(numImg)\n",
    "#         X_train = X_train[:, permutation]\n",
    "#         Y_train = y_train[:, permutation]\n",
    "        X_train = X_train\n",
    "        Y_train = y_train\n",
    "        batches = numImg//batchSize\n",
    "        \n",
    "        for batchNum in range(batches):\n",
    "            startBatchFrom = batchNum * batchSize\n",
    "            endBatchAt = min(startBatchFrom + batchSize, numImg - 1)\n",
    "#             print(startBatchFrom, endBatchAt)\n",
    "            currX = X_train[:, startBatchFrom: endBatchAt+1]\n",
    "            currY = Y_train[:, startBatchFrom: endBatchAt+1] \n",
    "#             print(currY)\n",
    "\n",
    "            outputForwardPropagation = forwardProp(currX)\n",
    "#             print(outputForwardPropagation[2][1])\n",
    "            weightD, biasD = backProp(currX, currY, outputForwardPropagation,batchSize) #endBatchAt - startBatchFrom)\n",
    "            \n",
    "            weightsAndBiases['W1'] = weightsAndBiases['W1'] - (learningRate * weightD[0])\n",
    "            weightsAndBiases['W2'] = weightsAndBiases['W2'] - (learningRate * weightD[1])\n",
    "            weightsAndBiases['W3'] = weightsAndBiases['W3'] - (learningRate * weightD[2])\n",
    "            weightsAndBiases['b1'] = weightsAndBiases['b1'] - (learningRate * biasD[0])\n",
    "            weightsAndBiases['b2'] = weightsAndBiases['b2'] - (learningRate * biasD[1])\n",
    "            weightsAndBiases['b3'] = weightsAndBiases['b3'] - (learningRate * biasD[2])\n",
    "#             print(weightsAndBiases['W1'])\n",
    "        outputForwardPropagation = forwardProp(X_train)\n",
    "#         print(outputForwardPropagation[2][1])\n",
    "        lossCurrentEpochTrain = CalculateLoss(outputForwardPropagation[2][1], Y_train)\n",
    "        \n",
    "            \n",
    "        outputForwardPropagationTest = forwardProp(X_test)\n",
    "        lossCurrentEpochTest = CalculateLoss(outputForwardPropagationTest[2][1], y_test)\n",
    "        print(\"Accuracy: \", accuracyScore(X_test, y_test))\n",
    "        print(\"Epoch {}: training loss = {}, test loss = {}\".format(\n",
    "        epoch + 1, lossCurrentEpochTrain, lossCurrentEpochTest))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ac7bedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num:  60000\n",
      "Accuracy:  10.280000000000001\n",
      "Epoch 1: training loss = 3.2499779666889834, test loss = 3.246307943074507\n",
      "Accuracy:  10.280000000000001\n",
      "Epoch 2: training loss = 3.249461062708267, test loss = 3.239908422812905\n",
      "Accuracy:  20.580000000000002\n",
      "Epoch 3: training loss = 3.2481448384843037, test loss = 3.225062850338533\n",
      "Accuracy:  26.200000000000003\n",
      "Epoch 4: training loss = 3.2396724353057746, test loss = 3.1654980797964076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3v/9bkbypyn6mxc9jk17t5t72580000gn/T/ipykernel_10131/1559785344.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1. / (1. + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  22.8\n",
      "Epoch 5: training loss = 2.8501920141905517, test loss = 3.940528168716593\n",
      "Accuracy:  35.9\n",
      "Epoch 6: training loss = 2.114559789854537, test loss = 3.6216421461699997\n",
      "Accuracy:  46.800000000000004\n",
      "Epoch 7: training loss = 1.8240146760469316, test loss = 2.99172696261881\n",
      "Accuracy:  63.580000000000005\n",
      "Epoch 8: training loss = 1.3750093755486055, test loss = 2.21066245570746\n",
      "Accuracy:  68.42\n",
      "Epoch 9: training loss = 1.1821068519496045, test loss = 1.8767036911494885\n",
      "Accuracy:  73.66\n",
      "Epoch 10: training loss = 1.0685920874231445, test loss = 1.4941809255063603\n",
      "Accuracy:  79.91\n",
      "Epoch 11: training loss = 0.9648350984776843, test loss = 1.1923878849870433\n",
      "Accuracy:  81.99\n",
      "Epoch 12: training loss = 0.8781718632864356, test loss = 1.1107089206964775\n",
      "Accuracy:  81.52000000000001\n",
      "Epoch 13: training loss = 0.8130295051512171, test loss = 1.1597366293685785\n",
      "Accuracy:  81.83\n",
      "Epoch 14: training loss = 0.7608228394645913, test loss = 1.1837032890404118\n",
      "Accuracy:  82.94\n",
      "Epoch 15: training loss = 0.7127236414984112, test loss = 1.1519816578032656\n",
      "Accuracy:  84.6\n",
      "Epoch 16: training loss = 0.6650363222542065, test loss = 1.0756691489804056\n",
      "Accuracy:  85.99\n",
      "Epoch 17: training loss = 0.6207887966323223, test loss = 0.9728141379755658\n",
      "Accuracy:  86.91\n",
      "Epoch 18: training loss = 0.5840687907108753, test loss = 0.8876288784825027\n",
      "Accuracy:  87.92\n",
      "Epoch 19: training loss = 0.5543101375157613, test loss = 0.8207165937095624\n",
      "Accuracy:  88.55\n",
      "Epoch 20: training loss = 0.5292907551217398, test loss = 0.7673313331600153\n",
      "Accuracy:  89.42\n",
      "Epoch 21: training loss = 0.5073021415059693, test loss = 0.722434010510712\n",
      "Accuracy:  89.94\n",
      "Epoch 22: training loss = 0.4872987191857565, test loss = 0.6804178101041225\n",
      "Accuracy:  90.48\n",
      "Epoch 23: training loss = 0.46868899986387197, test loss = 0.6421396108926624\n",
      "Accuracy:  90.98\n",
      "Epoch 24: training loss = 0.45114881207940916, test loss = 0.6062527299657366\n",
      "Accuracy:  91.32000000000001\n",
      "Epoch 25: training loss = 0.43450169913066894, test loss = 0.5762491076502078\n",
      "Accuracy:  91.64999999999999\n",
      "Epoch 26: training loss = 0.41865118526075124, test loss = 0.5510496967913019\n",
      "Accuracy:  91.9\n",
      "Epoch 27: training loss = 0.4035438594194643, test loss = 0.5331631543946204\n",
      "Accuracy:  92.10000000000001\n",
      "Epoch 28: training loss = 0.38914914229117853, test loss = 0.5159696631778882\n",
      "Accuracy:  92.22\n",
      "Epoch 29: training loss = 0.3754476246409031, test loss = 0.5058633790018809\n",
      "Accuracy:  92.24\n",
      "Epoch 30: training loss = 0.36242390680367775, test loss = 0.4933852071354879\n",
      "Accuracy:  92.21000000000001\n",
      "Epoch 31: training loss = 0.3500624540380336, test loss = 0.4871798257702772\n",
      "Accuracy:  92.25\n",
      "Epoch 32: training loss = 0.33834579168853895, test loss = 0.480457468537104\n",
      "Accuracy:  92.43\n",
      "Epoch 33: training loss = 0.32725397594829214, test loss = 0.47676714268284326\n",
      "Accuracy:  92.4\n",
      "Epoch 34: training loss = 0.3167643164819142, test loss = 0.475572800749553\n",
      "Accuracy:  92.19000000000001\n",
      "Epoch 35: training loss = 0.3068511165411822, test loss = 0.4739391625862406\n",
      "Accuracy:  92.24\n",
      "Epoch 36: training loss = 0.2974857251627182, test loss = 0.47547084192312916\n",
      "Accuracy:  92.19000000000001\n",
      "Epoch 37: training loss = 0.2886371000528163, test loss = 0.47944700021528747\n",
      "Accuracy:  92.08\n",
      "Epoch 38: training loss = 0.28027278002257144, test loss = 0.48521773244978034\n",
      "Accuracy:  91.95\n",
      "Epoch 39: training loss = 0.27235998980349874, test loss = 0.4924336439851056\n",
      "Accuracy:  91.95\n",
      "Epoch 40: training loss = 0.2648666024229861, test loss = 0.5004392109733432\n",
      "Accuracy:  91.75\n",
      "Epoch 41: training loss = 0.25776181246749585, test loss = 0.5062262797154475\n",
      "Accuracy:  91.64\n",
      "Epoch 42: training loss = 0.2510165277983168, test loss = 0.5142702909768393\n",
      "Accuracy:  91.5\n",
      "Epoch 43: training loss = 0.24460357043218361, test loss = 0.5246575923420198\n",
      "Accuracy:  91.19\n",
      "Epoch 44: training loss = 0.23849776814320525, test loss = 0.5324865371806757\n",
      "Accuracy:  91.01\n",
      "Epoch 45: training loss = 0.23267597206341814, test loss = 0.5440677955085712\n",
      "Accuracy:  90.8\n",
      "Epoch 46: training loss = 0.22711700956680117, test loss = 0.5547351309518824\n",
      "Accuracy:  90.64999999999999\n",
      "Epoch 47: training loss = 0.22180158437086267, test loss = 0.5659774774037426\n",
      "Accuracy:  90.51\n",
      "Epoch 48: training loss = 0.2167121446759435, test loss = 0.577246320638994\n",
      "Accuracy:  90.28\n",
      "Epoch 49: training loss = 0.21183274004726457, test loss = 0.5911461191195299\n",
      "Accuracy:  90.03\n",
      "Epoch 50: training loss = 0.2071488799740563, test loss = 0.6043211527077749\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "batch_size = 64\n",
    "learningRate = 0.01\n",
    "# print(y_train.shape)\n",
    "# print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "train(X_train, y_train, X_test, y_test, epochs, batch_size, learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27141554",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b52e06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
