{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baa85158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec7e6279",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('train_image.csv', header = None)\n",
    "training_label = pd.read_csv('train_label.csv', header = None)\n",
    "test_data = pd.read_csv('test_image.csv', header = None)\n",
    "test_label = pd.read_csv('test_label.csv', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a57cb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(training_data)\n",
    "y = np.array(training_label)\n",
    "X_test = np.array(test_data)\n",
    "y_test = np.array(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79079fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[0].reshape((28,28)), cmap='gray')\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2c5230d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = (X/255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "618ae695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotEncode(y):\n",
    "    ans = []\n",
    "    for i in range(len(y)):\n",
    "        currentLabel = np.zeros(10)\n",
    "        currentLabel[y[i]] = 1\n",
    "        ans.append(currentLabel)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6523f4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "y_train_encoded = oneHotEncode(y)\n",
    "y_test_encoded = oneHotEncode(y_test)\n",
    "print(np.array(y_train_encoded).shape, np.array(y_test_encoded).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "996727ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_encoded = oneHotEncode(y)\n",
    "# print(np.array(y_encoded).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02d65172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1. / (1. + np.exp(-x))\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x - x.max()) / np.sum(np.exp(x - x.max()), axis = 0, keepdims = True)\n",
    "\n",
    "def CalculateLoss(yHat, y):\n",
    "#     yHat = yHat*y\n",
    "    L_sum = np.sum(-1 * np.multiply(y, np.log(yHat + 1e-8)))\n",
    "#     L_sum = np.sum(np.multiply(y, np.log(yHat)) + np.multiply((1 - y), np.log(1 - yHat)))\n",
    "    m = y.shape[1]\n",
    "    L = (1./m) * L_sum\n",
    "\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52184f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(X, y, trainSize = 0.85):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    lenTrain = len(X) * trainSize\n",
    "    X_copy = list(X)\n",
    "    y_copy = list(y)\n",
    "    while len(X_train) < lenTrain:\n",
    "        ind = randrange(len(X_copy))\n",
    "        X_train.append(X_copy.pop(ind))\n",
    "        y_train.append(y_copy.pop(ind))\n",
    "    return X_train, X_copy, y_train, y_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e46dde30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = splitData(X, y_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "753062fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "#     Run this cell only once.\n",
    "#     Else, it will transpose X multiple times and we won't get the desired results.\n",
    "# \"\"\"\n",
    "# X_train = (np.array(X_train)).T\n",
    "# y_train = (np.array(y_train)).T\n",
    "# X_test = (np.array(X_test)).T\n",
    "# y_test = (np.array(y_test)).T\n",
    "# print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "# print(np.random.permutation(X_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65810ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 60000) (10, 60000) (784, 10000) (10, 10000)\n",
      "[ 6366   977 42051 ... 40393 43884 23316]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Run this cell only once.\n",
    "    Else, it will transpose X multiple times and we won't get the desired results.\n",
    "\"\"\"\n",
    "X_train = (np.array(X)).T\n",
    "y_train = (np.array(y_train_encoded)).T\n",
    "X_test = (np.array(X_test)).T\n",
    "y_test = (np.array(y_test_encoded)).T\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "print(np.random.permutation(X_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88eb77ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "nnL1 = 512\n",
    "nnL2 = 256\n",
    "numFeat = 784\n",
    "classes = 10\n",
    "weightsAndBiases = {\n",
    "    'W1' : np.random.randn(nnL1, numFeat) * 0.01,\n",
    "    'W2' : np.random.randn(nnL2, nnL1) * 0.01,\n",
    "    'W3' : np.random.randn(classes, nnL2) * 0.01,\n",
    "    'b1' : np.zeros(nnL1).reshape(nnL1,1),\n",
    "    'b2' : np.zeros(nnL2).reshape(nnL2,1),\n",
    "    'b3' : np.zeros(classes).reshape(classes,1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "949fdfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardProp(X):\n",
    "#     print(weightsAndBiases)\n",
    "    W1 = weightsAndBiases['W1']\n",
    "    W2 = weightsAndBiases['W2']\n",
    "    W3 = weightsAndBiases['W3']\n",
    "    b1 = weightsAndBiases['b1']\n",
    "    b2 = weightsAndBiases['b2']\n",
    "    b3 = weightsAndBiases['b3']\n",
    "#     print(W1.shape, X.shape, b1.shape)\n",
    "    z1 = np.matmul(W1, X) + b1\n",
    "    o1 = sigmoid(z1)\n",
    "#     print(outputLayer1)\n",
    "\n",
    "#     print(z1.shape)\n",
    "    z2 = np.matmul(W2, o1) + b2\n",
    "    o2 = sigmoid(z2)\n",
    "    z3 = np.matmul(W3, o2) + b3\n",
    "    o3 = softmax(z3)\n",
    "    importantValuesForBackPropagation = []\n",
    "    importantValuesForBackPropagation.append((z1, o1))\n",
    "    importantValuesForBackPropagation.append((z2, o2))\n",
    "    importantValuesForBackPropagation.append((z3, o3))\n",
    "    \n",
    "    return importantValuesForBackPropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6446024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backProp(X, Y, z_a_values, batchSize):\n",
    "    yHat = z_a_values[2][1]\n",
    "    O2 = z_a_values[1][1]\n",
    "    z2 = z_a_values[1][0]\n",
    "    z1 = z_a_values[0][0]\n",
    "    O1 = z_a_values[0][1]\n",
    "#     loss = CrossEntropy(yHat, Y)\n",
    "    \"\"\"\n",
    "    https://towardsdatascience.com/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1\n",
    "    dLossByZ3 = (dCost/dOut3) * (dOut3/dZ3)\n",
    "    dLossBydW3 = dLossBydOutput * (dZ3/dW3)\n",
    "    \n",
    "    dLossBydOutput2 = dLossBydOutput * (dZ3/dOut2)\n",
    "    dLossBydW2 = dLossBydOutput2 * (dOut2/dZ2) * (dZ2/dW2)  [dOut2/dZ2 = sigmoid(z2) * (1-sigmoid(z2))]\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    dZ3 = yHat - Y #+ (1-Y)/(1-yHat)\n",
    "    \n",
    "#     print(dLossBydOutput.shape)\n",
    "#     print(outputLayer2.T.shape)\n",
    "    \n",
    "    dW3 = (1/batchSize) * np.matmul(dZ3, O2.T)\n",
    "    db3 = (1/batchSize) * np.sum(dZ3, axis=1, keepdims=True)\n",
    "    \n",
    "    \n",
    "    dO2 = np.dot(weightsAndBiases['W3'].T, dZ3)\n",
    "    \n",
    "    \n",
    "#     print(dLossBydOutput2.shape, sigmoid(z2).shape)\n",
    "\n",
    "    dZ2 = dO2 * sigmoid(z2) * (1-sigmoid(z2))\n",
    "    \n",
    "    dW2 = (1/batchSize) * np.matmul(dZ2, O1.T)\n",
    "    db2 = (1/batchSize) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "        \n",
    "#     print(dLossBydOutput2.shape, dO2BydZ2.shape)\n",
    "#     dLossBydOutput1 = (dLossBydOutput2 * dO2BydZ2)\n",
    "\n",
    "    dO1 = np.dot(weightsAndBiases['W2'].T, dZ2) \n",
    "    dZ1 = dO1 * sigmoid(z1) * (1-sigmoid(z1))\n",
    "\n",
    "    \n",
    "    dW1 = (1/batchSize) * np.matmul(dZ1, X.T)\n",
    "    db1 = (1/batchSize) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    weightDerivatives = [dW1, dW2, dW3]\n",
    "    biasDerivatives = [db1, db2, db3]\n",
    "    \n",
    "    return weightDerivatives, biasDerivatives\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f2758ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracyScore(x,y):\n",
    "    yHat = forwardProp(x)\n",
    "    yHat = yHat[2][1]\n",
    "    yHat = yHat.argmax(axis=0)\n",
    "    y = y.argmax(axis=0)\n",
    "    c1 = (yHat==y)*1\n",
    "    return np.mean(c1) * 100\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a984028a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, y_train, X_test, y_test, epochs, batchSize, learningRate):\n",
    "    numImg = X_train.shape[1]\n",
    "    print('num: ',numImg)\n",
    "    for epoch in range(epochs):\n",
    "        X_train = X_train\n",
    "        Y_train = y_train\n",
    "        batches = numImg//batchSize\n",
    "        \n",
    "        for batchNum in range(batches):\n",
    "            startBatchFrom = batchNum * batchSize\n",
    "            endBatchAt = min(startBatchFrom + batchSize, numImg - 1)\n",
    "#             print(startBatchFrom, endBatchAt)\n",
    "            currX = X_train[:, startBatchFrom: endBatchAt+1]\n",
    "            currY = Y_train[:, startBatchFrom: endBatchAt+1] \n",
    "#             print(currY)\n",
    "\n",
    "            outputForwardPropagation = forwardProp(currX)\n",
    "#             print(outputForwardPropagation[2][1])\n",
    "            weightD, biasD = backProp(currX, currY, outputForwardPropagation,batchSize) #endBatchAt - startBatchFrom)\n",
    "            \n",
    "            weightsAndBiases['W1'] = weightsAndBiases['W1'] - (learningRate * weightD[0])\n",
    "            weightsAndBiases['W2'] = weightsAndBiases['W2'] - (learningRate * weightD[1])\n",
    "            weightsAndBiases['W3'] = weightsAndBiases['W3'] - (learningRate * weightD[2])\n",
    "            weightsAndBiases['b1'] = weightsAndBiases['b1'] - (learningRate * biasD[0])\n",
    "            weightsAndBiases['b2'] = weightsAndBiases['b2'] - (learningRate * biasD[1])\n",
    "            weightsAndBiases['b3'] = weightsAndBiases['b3'] - (learningRate * biasD[2])\n",
    "#             print(weightsAndBiases['W1'])\n",
    "        outputForwardPropagation = forwardProp(X_train)\n",
    "#         print(outputForwardPropagation[2][1])\n",
    "        lossCurrentEpochTrain = CalculateLoss(outputForwardPropagation[2][1], Y_train)\n",
    "        \n",
    "            \n",
    "        outputForwardPropagationTest = forwardProp(X_test)\n",
    "        lossCurrentEpochTest = CalculateLoss(outputForwardPropagationTest[2][1], y_test)\n",
    "        print(\"Accuracy: \", accuracyScore(X_test, y_test))\n",
    "        print(\"Epoch {}: training loss = {}, test loss = {}\".format(\n",
    "        epoch + 1, lossCurrentEpochTrain, lossCurrentEpochTest))\n",
    "    return outputForwardPropagationTest\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ac7bedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num:  60000\n",
      "Accuracy:  88.09\n",
      "Epoch 1: training loss = 0.4935299724093144, test loss = 0.4870150456995266\n",
      "Accuracy:  91.5\n",
      "Epoch 2: training loss = 0.3140011373768072, test loss = 0.3109807928756103\n",
      "Accuracy:  92.80000000000001\n",
      "Epoch 3: training loss = 0.25593457450249063, test loss = 0.2552233530834686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3v/9bkbypyn6mxc9jk17t5t72580000gn/T/ipykernel_14087/2284288791.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1. / (1. + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  93.73\n",
      "Epoch 4: training loss = 0.22361327316379961, test loss = 0.2259443067786238\n",
      "Accuracy:  93.96\n",
      "Epoch 5: training loss = 0.20264170748035684, test loss = 0.21113363474409677\n",
      "Accuracy:  94.71000000000001\n",
      "Epoch 6: training loss = 0.18393241232619634, test loss = 0.18917139925579257\n",
      "Accuracy:  94.78\n",
      "Epoch 7: training loss = 0.17665358629519098, test loss = 0.18438180166309268\n",
      "Accuracy:  94.78\n",
      "Epoch 8: training loss = 0.17553363890890922, test loss = 0.18094319179463647\n",
      "Accuracy:  95.05\n",
      "Epoch 9: training loss = 0.16633768234317717, test loss = 0.17434102238549737\n",
      "Accuracy:  94.93\n",
      "Epoch 10: training loss = 0.16624390631861538, test loss = 0.17470143649913802\n",
      "Accuracy:  94.89\n",
      "Epoch 11: training loss = 0.16681027615591473, test loss = 0.1741823773344723\n",
      "Accuracy:  94.98\n",
      "Epoch 12: training loss = 0.16544936152689488, test loss = 0.17650935473028126\n",
      "Accuracy:  95.39999999999999\n",
      "Epoch 13: training loss = 0.1575754267971187, test loss = 0.16518710063047165\n",
      "Accuracy:  95.07\n",
      "Epoch 14: training loss = 0.1580156912843544, test loss = 0.16923382615758478\n",
      "Accuracy:  95.19\n",
      "Epoch 15: training loss = 0.1512622250102339, test loss = 0.16289123116764828\n",
      "Accuracy:  95.04\n",
      "Epoch 16: training loss = 0.1565658829156391, test loss = 0.17311967864465755\n",
      "Accuracy:  95.27\n",
      "Epoch 17: training loss = 0.14646061370284844, test loss = 0.1581472989773599\n",
      "Accuracy:  95.04\n",
      "Epoch 18: training loss = 0.15521924941251, test loss = 0.1626109857312745\n",
      "Accuracy:  95.39\n",
      "Epoch 19: training loss = 0.14574226413051405, test loss = 0.15498304645150587\n",
      "Accuracy:  94.99\n",
      "Epoch 20: training loss = 0.15331290527717267, test loss = 0.16440385494171272\n",
      "Accuracy:  95.05\n",
      "Epoch 21: training loss = 0.15641678352331223, test loss = 0.16535319394635442\n",
      "Accuracy:  94.84\n",
      "Epoch 22: training loss = 0.1630196726993926, test loss = 0.17350647182392878\n",
      "Accuracy:  94.98\n",
      "Epoch 23: training loss = 0.15577292965733655, test loss = 0.16919331989980402\n",
      "Accuracy:  95.08\n",
      "Epoch 24: training loss = 0.15099929271517565, test loss = 0.16142674100091006\n",
      "Accuracy:  95.0\n",
      "Epoch 25: training loss = 0.15205583219488447, test loss = 0.1621082913728826\n",
      "Accuracy:  94.78999999999999\n",
      "Epoch 26: training loss = 0.15782682862202752, test loss = 0.16988231877032198\n",
      "Accuracy:  94.61\n",
      "Epoch 27: training loss = 0.16173714764160083, test loss = 0.18299588365702985\n",
      "Accuracy:  94.83\n",
      "Epoch 28: training loss = 0.1583459691413187, test loss = 0.16758203803247218\n",
      "Accuracy:  95.12\n",
      "Epoch 29: training loss = 0.14669260678094645, test loss = 0.15624783874112416\n",
      "Accuracy:  95.25\n",
      "Epoch 30: training loss = 0.14441319394921828, test loss = 0.15634393884754072\n",
      "Accuracy:  95.67\n",
      "Epoch 31: training loss = 0.13895521085422938, test loss = 0.1456015756854166\n",
      "Accuracy:  95.28\n",
      "Epoch 32: training loss = 0.14001836141549096, test loss = 0.15288322506702492\n",
      "Accuracy:  95.32000000000001\n",
      "Epoch 33: training loss = 0.14262159216998577, test loss = 0.1519549886652168\n",
      "Accuracy:  95.24000000000001\n",
      "Epoch 34: training loss = 0.14898890972802564, test loss = 0.15444165360307885\n",
      "Accuracy:  94.74000000000001\n",
      "Epoch 35: training loss = 0.15611857781404087, test loss = 0.17146923044519757\n",
      "Accuracy:  95.22\n",
      "Epoch 36: training loss = 0.14407430965266801, test loss = 0.15876754302683574\n",
      "Accuracy:  95.15\n",
      "Epoch 37: training loss = 0.1530802217761349, test loss = 0.1603813114899773\n",
      "Accuracy:  95.23\n",
      "Epoch 38: training loss = 0.14360912455122266, test loss = 0.15627723928763265\n",
      "Accuracy:  94.96\n",
      "Epoch 39: training loss = 0.147680609761263, test loss = 0.1615144161023782\n",
      "Accuracy:  94.99\n",
      "Epoch 40: training loss = 0.14297288656753593, test loss = 0.15806924122526403\n",
      "Accuracy:  94.78\n",
      "Epoch 41: training loss = 0.14702790772763605, test loss = 0.16130148807670558\n",
      "Accuracy:  95.22\n",
      "Epoch 42: training loss = 0.1413029746355285, test loss = 0.15647307505607522\n",
      "Accuracy:  95.37\n",
      "Epoch 43: training loss = 0.14070860492294168, test loss = 0.15576495910601665\n",
      "Accuracy:  95.26\n",
      "Epoch 44: training loss = 0.13625419821794768, test loss = 0.15637570812882717\n",
      "Accuracy:  95.03\n",
      "Epoch 45: training loss = 0.13894303609115954, test loss = 0.156758946887867\n",
      "Accuracy:  94.89\n",
      "Epoch 46: training loss = 0.1439575360707806, test loss = 0.1647969743462162\n",
      "Accuracy:  95.46\n",
      "Epoch 47: training loss = 0.13840299777516626, test loss = 0.1504207580704968\n",
      "Accuracy:  94.89999999999999\n",
      "Epoch 48: training loss = 0.15430740282331623, test loss = 0.1719904347713897\n",
      "Accuracy:  94.75\n",
      "Epoch 49: training loss = 0.14480148421758202, test loss = 0.16470524804049472\n",
      "Accuracy:  95.24000000000001\n",
      "Epoch 50: training loss = 0.13485060972603866, test loss = 0.15536028132501115\n",
      "Accuracy:  94.63000000000001\n",
      "Epoch 51: training loss = 0.15363479974603136, test loss = 0.17168541532375103\n",
      "Accuracy:  94.99\n",
      "Epoch 52: training loss = 0.14983867130536516, test loss = 0.162755307735993\n",
      "Accuracy:  94.86\n",
      "Epoch 53: training loss = 0.14304389126769113, test loss = 0.16481321321490347\n",
      "Accuracy:  95.35\n",
      "Epoch 54: training loss = 0.13573607581387573, test loss = 0.14829904459567406\n",
      "Accuracy:  94.89999999999999\n",
      "Epoch 55: training loss = 0.14107347450022723, test loss = 0.15969426517466015\n",
      "Accuracy:  94.93\n",
      "Epoch 56: training loss = 0.13648806686088022, test loss = 0.1555132219043343\n",
      "Accuracy:  94.38\n",
      "Epoch 57: training loss = 0.15728419328203755, test loss = 0.1793240649876265\n",
      "Accuracy:  94.87\n",
      "Epoch 58: training loss = 0.14234087822202268, test loss = 0.15965968825184204\n",
      "Accuracy:  94.88\n",
      "Epoch 59: training loss = 0.1444258209184868, test loss = 0.1612637571282445\n",
      "Accuracy:  95.58\n",
      "Epoch 60: training loss = 0.12794066553183517, test loss = 0.1432461289185527\n"
     ]
    }
   ],
   "source": [
    "epochs = 60\n",
    "batch_size = 16\n",
    "learningRate = 0.0075\n",
    "# print(y_train.shape)\n",
    "# print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "pred = train(X_train, y_train, X_test, y_test, epochs, batch_size, learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27141554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.01785222e-07 1.40134206e-05 1.20794943e-10 ... 2.63417287e-05\n",
      "  3.05831590e-05 6.17176607e-04]\n",
      " [3.11021038e-06 8.14388047e-05 9.99787098e-01 ... 1.02806148e-06\n",
      "  7.99493811e-04 4.17269582e-07]\n",
      " [3.07472452e-05 9.99361922e-01 4.17734658e-05 ... 7.64876869e-07\n",
      "  4.50664556e-06 5.59015371e-04]\n",
      " ...\n",
      " [9.99849371e-01 2.43935096e-08 1.38309615e-04 ... 3.15213339e-04\n",
      "  1.03173284e-05 4.44614892e-08]\n",
      " [5.23757444e-08 1.14856850e-04 9.70083445e-06 ... 7.53570647e-05\n",
      "  5.45652087e-03 3.85311311e-05]\n",
      " [4.26576855e-06 2.95343784e-11 2.65586989e-08 ... 1.02167441e-02\n",
      "  7.58900397e-06 1.02743215e-06]]\n"
     ]
    }
   ],
   "source": [
    "pred = pred[2][1]\n",
    "print(pred)\n",
    "pred = np.argmax(pred , axis= 0 )\n",
    "df = pd.DataFrame(pred)\n",
    "df.to_csv('test_predictions.csv', index = False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b52e06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
